\chapter{Discussion}
\label{ch:discussion}

\textit{The maximum throughput for different cluster configurations was found by load testing a test application with a test tool developed specifically for the test. The previous chapter showed that improving capacity and redundancy guaranteed availability at higher loads, thereby showing that improvements to capacity improve system resilience. This chapter will discuss the test results and introduce a test that could have shown the relation between fault tolerance and avoidance and availability, showing how the relationship affects the resilience of a system. Furthermore, the challenges for enterprises in utilizing open source projects will be discussed.}

It has previously been defined how resilience is determined by the adaptivity of the system components and quantified by evaluating the dependability. A test was completed to show how service capacity has a clear influence on availability. By improving the capacity, better guarantees for the availability can be defined.

Through several design and development iterations, limitations in the test application and test tool were identified and solved, especially the test tools influence on the test results were eliminated. Unidentified limitations in the test application still made it impossible to achieve a 1:1 relation between increasing capacity and maximum throughput. It has been concluded that the load balancer created with Kubernetes possibly acted as a bottleneck for the test. 

This chapter will comment further on the test measuring availability and expand with another possible test related to resilience. Furthermore, existing challenges with adoption of open source projects in big enterprise organisations will be discussed.

\section{Measuring Availability}
Designing and performing an experiment incorporates many different aspects in both test application, test tool and the utilized infrastructure. Taking all aspects into account when designing experiments is therefore incredibly hard. Only through many iterations it becomes possible to achieve a setup that truly shows the limitations in the test application or the infrastructure it is deployed upon. The availability test showed limitations in some parts of the system in all cluster configurations. Extensive monitoring of the setup showed that lack of processing power on participating nodes was a problem reserved to a 1 node cluster configuration. When running 5 and 10 node cluster configurations, some other aspects were the limiting factor. It was preliminarily assumed that the Kubernetes load balancer most likely was the system's new bottleneck, but due to time constrains this was not confirmed.

The availability test still confirmed that throughput was increased by increasing capacity, albeit with a suboptimal factor, not utilizing the available processor power on each individual node. This further illustrates how a simple load test can show problems with a particular setup before an application is put into production. By utilizing Kubernetes setting up the cluster and configuring it was a straightforward and quick task. This made it possible to reschedule test easily after re-factoring limiting aspects of test application and test tool. The test was designed, developed and initially tested locally in container environments, before being moved into an identical container environment in production, showing the power of having isolated environments.

The availability test could have been expanded in many different ways according to needs in an organisation. Some of the evaluated open source benchmarking tools support advanced scenario setups\footnote{Api benchmark (https://github.com/matteofigus/api-benchmark) and benchrest (https://github.com/jeffbski/bench-rest) were both evaluated but disregarded in favour of Vegeta.} where a flow including several GET or PUSH requests are done in a particular order, even incorporating responses from the test application received at a certain step (eg. an id of an POST request, then used to test a GET request on the newly created item). This serves as a powerful tool that could be used to immediate real traffic observed in a production environment, giving the possibility to imitate user flows observed in production. These scenarios could advantageously be automatically driven on any new commit or release candidate by incorporating the load test as part of a CI or CD pipeline.

\subsection{Fault Tolerance and Avoidance}
The circuit breaker pattern was implemented in a test application to investigate the relationship between fault tolerance and avoidance and availability. Two Spring Boot applications were developed to showcase how introducing a circuit breaker in an integration point can mitigate cascading failures. The Spring Boot framework has a library containing the Netflix Hystrix circuit breaker which was utilized to implement the circuit breaker pattern in the integration point between the two services.

By setting up a test environment, where both services could be controlled (especially turning off the down stream service), it would have been possible to showcase how availability in a service with downstream decencies is at risk of being drastically impaired without safety measures. Furthermore, the introduction of the circuit breaker safety measure could have shown how availability can be uphold if downstream services are correctly safeguarded. 

The circuit breaker prevents cascading failure by short-circuiting the integration point and thereby ensuring a fast failure response having no impact on the availability. A failure in a downstream service still incurs functionality degrading due to a lack of information from a downstream service. But failing fast and degrading functionality accordingly is still an improvement. By stopping failure propagation, reducing stress on failing downstream services and eliminating useless network traffic.

\note {
\section{Aggregating Data}
By distributing an application many different facets of software engineering is in play, Cloud Computing has made it possible to gain further insight into how applications are running, by providing extensive monitoring solutions, but having the necessary data to deduct where a problems stems from can still be incredible hard. 


Gathering data is easy, aggregating and determining which data is important and why is hard. Kubernetes makes a lot of data available, on many different levels of the system stack, making it easy to get data, but still hard to choose the important data. Gathering data does not solve the inherent problem with distributed systems: lack of transparency. Having access to the data on the other hand makes us able to solve this problem, which has always existed, also in monolith or SOA architectures. Deploying a applications on a distant server does inherently make it hard to reason about the application behaviour, but having easy access to data that is intermediate between microservices makes it possible to reason about the behaviour, be it still hard, but possible with the correct aggregation tools.

This shows how performance of the individual application is a medley of all stack levels: Infrastructure, provisioning, runtime, orchestration and management, and application definition and development. Stressing the need for a centralization of the responsibility and focus of all the layers, Spotify is doing it with great success already, it makes a lot of sense when evaluated from this perspective. Neither of the prohibiting factors should be in place no matter what architecture is chose, if developers are limited by organisation, process or technology adoption, the end product will be as well.
}


\section{Open Source Projects}
Many open source projects make advanced frameworks and tools available increasing the speed tremendously of software development. Having these open source projects at hand is a major benefit for software developers. This new level of available technology is one of Evans' main arguments when stating that good DDD is easier attainable and therefore more relevant than ever before \cite[t.~28:21]{evans2016tackling}. Netflix's Meshenberg argues that the Netflix cloud adoption was hindered by the need for designing and implementing the very fundamental microservice enabling technologies in-house. Emphasizing how new cloud adopters should utilize the now freely available tools if ever considering moving to the cloud and utilizing a microservice architecture \cite[t.~38:18]{meshenberg2016microservices}. According to George, lack of technology adoption is one of the three key inhibitors for adopting microservices \cite[t.~12:45]{george2016it}.

At the same time, open source projects are indeed very vulnerable. No guarantees exist when relying on continues community contributions adding features, and more importantly fixing bugs. Moreover, existing functionality might not survive updates, breaking changes to open source projects is a known issue for Docker\footnote{The Docker documentation specifically talking about breaking changes \url{https://docs.docker.com/engine/breaking_changes/}}. Kubernetes also introduced breaking changes to the access policies which cause issues (which it did for this project as well) when trying to access monitoring tools in a running cluster\footnote{Kubernetes not allowing 'unprivileged access' to subcomponents of the running monitoring tools: \url{https://github.com/kubernetes/kubernetes/issues/39722}}, an issue which still has not been fixed after several months of being 'open' on the community page.

Having access to a high amount of open source tools is obviously a very big advantage to the modern developer. Creating resilient and feature rich systems have never been simpler, but utilizing open source also incurs some risks. Open source projects could be abandoned by the contributors without notice, or introduce breaking changes in an upcoming update. These are limiting factors for adoption of open source projects by big enterprises like Danske Bank. By being obligated, maybe by legislation, to deliver resilient and fully functional and secure systems that can be continuously developed in the future might create an incentive for enterprises to neglect the big opportunities and benefits of utilizing open source software.